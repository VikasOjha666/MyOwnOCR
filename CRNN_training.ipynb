{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CRNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6-KPHZ91zNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a8d3ad-9f63-4186-9fc2-b6a9c35316f5"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq-IHfAQ15A7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa3ffed-b203-4ee3-c8c4-d7f491ddf8f0"
      },
      "source": [
        "!git clone https://github.com/VikasOjha666/Data-generator-for-CRNN.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Data-generator-for-CRNN'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 37 (delta 14), reused 32 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Data-generator-for-CRNN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF8dOm49v1zE",
        "outputId": "36a34d68-32fc-444c-b51c-8f4f5fd77aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Data-generator-for-CRNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYT7zH6n394M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e9cbb2-8185-4662-d0e6-57db7d9dfebc"
      },
      "source": [
        "!mkdir images/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘images/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL_25u1_4DnB"
      },
      "source": [
        "!python generate_data.py --n_samples 300000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwcIFwjg4IN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b2035b-40ad-46b6-a809-e04096efc6b0"
      },
      "source": [
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.utils import shuffle\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNmZv6iY4Loh"
      },
      "source": [
        "char_list = string.ascii_letters+string.digits\n",
        " \n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst\n",
        "\n",
        "def find_dominant_color(image):\n",
        "        #Resizing parameters\n",
        "        width, height = 150,150\n",
        "        image = image.resize((width, height),resample = 0)\n",
        "        #Get colors from image object\n",
        "        pixels = image.getcolors(width * height)\n",
        "        #Sort them by count number(first element of tuple)\n",
        "        sorted_pixels = sorted(pixels, key=lambda t: t[0])\n",
        "        #Get the most frequent color\n",
        "        dominant_color = sorted_pixels[-1][1]\n",
        "        return dominant_color\n",
        "\n",
        "def preprocess_img(img, imgSize):\n",
        "    \"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
        "\n",
        "    # there are damaged files in IAM dataset - just use black image instead\n",
        "    if img is None:\n",
        "        img = np.zeros([imgSize[1], imgSize[0]]) \n",
        "        print(\"Image None!\")\n",
        "\n",
        "    # create target image and copy sample image into it\n",
        "    (wt, ht) = imgSize\n",
        "    (h, w) = img.shape\n",
        "    fx = w / wt\n",
        "    fy = h / ht\n",
        "    f = max(fx, fy)\n",
        "    newSize = (max(min(wt, int(w / f)), 1),\n",
        "               max(min(ht, int(h / f)), 1))  # scale according to f (result at least 1 and at most wt or ht)\n",
        "    img = cv2.resize(img, newSize, interpolation=cv2.INTER_CUBIC) # INTER_CUBIC interpolation best approximate the pixels image\n",
        "                                                               # see this https://stackoverflow.com/a/57503843/7338066\n",
        "    most_freq_pixel=find_dominant_color(Image.fromarray(img))\n",
        "    target = np.ones([ht, wt]) * most_freq_pixel  \n",
        "    target[0:newSize[1], 0:newSize[0]] = img\n",
        "\n",
        "    img = target\n",
        "\n",
        "    return img\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sic3Urxs4OkE"
      },
      "source": [
        "\n",
        "training_img = []\n",
        "training_txt = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "orig_txt = []\n",
        " \n",
        "#lists for validation dataset\n",
        "valid_img = []\n",
        "valid_txt = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_orig_txt = []\n",
        " \n",
        "max_label_len = 0\n",
        "\n",
        "annot=open('/content/Data-generator-for-CRNN/annotation.txt','r').readlines()\n",
        "imagenames=[]\n",
        "txts=[]\n",
        "\n",
        "for cnt in annot:\n",
        "    filename,txt=cnt.split(',')[0],cnt.split(',')[1].split('\\n')[0]\n",
        "    imagenames.append(filename)\n",
        "    txts.append(txt)\n",
        "    \n",
        "c = list(zip(imagenames, txts))\n",
        "\n",
        "random.shuffle(c)\n",
        "\n",
        "imagenames, txts = zip(*c)\n",
        "    \n",
        "\n",
        "    \n",
        "for i in range(len(imagenames)):\n",
        "        img = cv2.imread('/content/Data-generator-for-CRNN/images/'+imagenames[i],0)   \n",
        " \n",
        "        img=preprocess_img(img,(128,32))\n",
        "        img=np.expand_dims(img,axis=-1)\n",
        "        img = img/255.\n",
        "        txt = txts[i]\n",
        "        \n",
        "        # compute maximum length of the text\n",
        "        if len(txt) > max_label_len:\n",
        "            max_label_len = len(txt)\n",
        "            \n",
        "           \n",
        "        # split the 150000 data into validation and training dataset as 10% and 90% respectively\n",
        "        if i%10 == 0:     \n",
        "            valid_orig_txt.append(txt)   \n",
        "            valid_label_length.append(len(txt))\n",
        "            valid_input_length.append(31)\n",
        "            valid_img.append(img)\n",
        "            valid_txt.append(encode_to_labels(txt))\n",
        "        else:\n",
        "            orig_txt.append(txt)   \n",
        "            train_label_length.append(len(txt))\n",
        "            train_input_length.append(31)\n",
        "            training_img.append(img)\n",
        "            training_txt.append(encode_to_labels(txt)) \n",
        "        \n",
        "        # break the loop if total data is 150000\n",
        "        if i == 150000:\n",
        "            flag = 1\n",
        "            break\n",
        "        i+=1\n",
        "        \n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp-U2u0p4Y_T"
      },
      "source": [
        "#pad each output label to maximum text length\n",
        " \n",
        "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqWcDplF6HAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449c3359-389e-445b-d7cc-7bc682fd2328"
      },
      "source": [
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        "\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        "\n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "# model to be used at test time\n",
        "act_model = Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBtmnGNn6I0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4688c0-0540-47fa-e14c-d401d81ac38b"
      },
      "source": [
        "act_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 128, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 128, 64)       640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 32, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 32, 256)        295168    \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 32, 256)        590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 32, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 32, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 4, 32, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 32, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 1, 31, 512)        1049088   \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 31, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 31, 256)           656384    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 31, 256)           394240    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 31, 63)            16191     \n",
            "=================================================================\n",
            "Total params: 6,619,711\n",
            "Trainable params: 6,617,663\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMxpZweM6MfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe1580c-ef52-482b-dcf6-2a392523b4e3"
      },
      "source": [
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        " \n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhqs_1FE6NYR"
      },
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        " \n",
        "filepath=\"/content/best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LhU9Lpj6QU-"
      },
      "source": [
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length)\n",
        "train_label_length = np.array(train_label_length)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx41yVYP9tQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a5bfeb-a85d-4ccf-b948-90320e8af220"
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 15\n",
        "model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 135000 samples, validate on 15001 samples\n",
            "Epoch 1/15\n",
            "135000/135000 [==============================] - 382s 3ms/step - loss: 22.9777 - val_loss: 52.9860\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 52.98597, saving model to /content/best_model.hdf5\n",
            "Epoch 2/15\n",
            "135000/135000 [==============================] - 363s 3ms/step - loss: 1.0535 - val_loss: 0.8209\n",
            "\n",
            "Epoch 00002: val_loss improved from 52.98597 to 0.82091, saving model to /content/best_model.hdf5\n",
            "Epoch 3/15\n",
            "135000/135000 [==============================] - 363s 3ms/step - loss: 0.4901 - val_loss: 0.5557\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.82091 to 0.55570, saving model to /content/best_model.hdf5\n",
            "Epoch 4/15\n",
            "135000/135000 [==============================] - 362s 3ms/step - loss: 0.3424 - val_loss: 0.3525\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.55570 to 0.35247, saving model to /content/best_model.hdf5\n",
            "Epoch 5/15\n",
            "135000/135000 [==============================] - 362s 3ms/step - loss: 0.2676 - val_loss: 0.3250\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.35247 to 0.32497, saving model to /content/best_model.hdf5\n",
            "Epoch 6/15\n",
            "135000/135000 [==============================] - 362s 3ms/step - loss: 0.2217 - val_loss: 0.3562\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.32497\n",
            "Epoch 7/15\n",
            "135000/135000 [==============================] - 361s 3ms/step - loss: 0.1900 - val_loss: 0.3494\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.32497\n",
            "Epoch 8/15\n",
            "135000/135000 [==============================] - 360s 3ms/step - loss: 0.1681 - val_loss: 0.3286\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.32497\n",
            "Epoch 9/15\n",
            "135000/135000 [==============================] - 360s 3ms/step - loss: 0.1582 - val_loss: 0.4120\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.32497\n",
            "Epoch 10/15\n",
            "135000/135000 [==============================] - 360s 3ms/step - loss: 0.1501 - val_loss: 0.3074\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.32497 to 0.30741, saving model to /content/best_model.hdf5\n",
            "Epoch 11/15\n",
            "135000/135000 [==============================] - 360s 3ms/step - loss: 0.1244 - val_loss: 0.2460\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.30741 to 0.24602, saving model to /content/best_model.hdf5\n",
            "Epoch 12/15\n",
            "135000/135000 [==============================] - 360s 3ms/step - loss: 0.1160 - val_loss: 0.2314\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.24602 to 0.23145, saving model to /content/best_model.hdf5\n",
            "Epoch 13/15\n",
            "135000/135000 [==============================] - 360s 3ms/step - loss: 0.1052 - val_loss: 0.2242\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.23145 to 0.22422, saving model to /content/best_model.hdf5\n",
            "Epoch 14/15\n",
            "135000/135000 [==============================] - 361s 3ms/step - loss: 0.1003 - val_loss: 0.2518\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.22422\n",
            "Epoch 15/15\n",
            "135000/135000 [==============================] - 361s 3ms/step - loss: 0.1023 - val_loss: 0.2129\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.22422 to 0.21293, saving model to /content/best_model.hdf5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fa3515ef1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW4mPMB3MpCF"
      },
      "source": [
        "# load the saved best model weights\n",
        "act_model.load_weights('best_model.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(valid_img[10:20])\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        " \n",
        "# see the results\n",
        "i = 10\n",
        "for x in out:\n",
        "    print(\"original_text =  \", valid_orig_txt[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw6mKnt4MsqR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}